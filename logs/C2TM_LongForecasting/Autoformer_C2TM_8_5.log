Args in experiment:
Namespace(random_seed=2023, is_training=1, model_id='8_5', model='Autoformer', data='C2TM', root_path='./datas/C2TM/', data_path='ETTh1.csv', n_part=24, features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=8, pred_len=5, num_workers=10, itr=1, train_epochs=10, batch_size=128, patience=3, learning_rate=0.0001, weight_decay=0.0, des='test', loss='mse', lradj='type1', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, max_diffusion_step=2, cl_decay_steps=2000, filter_type='dual_random_walk', num_rnn_layers=2, rnn_units=64, use_curriculum_learning=False, patch_len=2, stride=2, fc_dropout=0.05, head_dropout=0.0, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, label_len=4, embed_type=0, enc_in=552, dec_in=552, c_out=552, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, version='Fourier', mode_select='random', modes=64, L=3, base='legendre', cross_activation='tanh')
Use GPU: cuda:0
>>>>>>>start training : 8_5_Autoformer_C2TM_ftM_sl8_ll4_pl5_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>
data shape:  (48, 13269) 13269
adj_mx shape:  (13269, 13269)
data shape:  (120, 13269) 13269
adj_mx shape:  (13269, 13269)
data shape:  (24, 13269) 13269
adj_mx shape:  (13269, 13269)
MSE Loss
	iters: 100, epoch: 1 | loss: 848.4189453
	speed: 0.0420s/iter; left time: 41.1620s
Epoch: 1 cost time: 4.4624316692352295
Epoch: 1, Steps: 108 | Train Loss: 168.2597319 Vali Loss: 11.7175665 Test Loss: 11.8259850
Validation loss decreased (inf --> 11.717566).  Saving model ...
not in xxxx...
type1 => Adjust updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 39.3464966
	speed: 0.0339s/iter; left time: 29.5570s
Epoch: 2 cost time: 2.6721389293670654
Epoch: 2, Steps: 108 | Train Loss: 143.2044549 Vali Loss: 11.6871023 Test Loss: 11.7783689
Validation loss decreased (11.717566 --> 11.687102).  Saving model ...
not in xxxx...
type1 => Adjust updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 16.5771999
	speed: 0.0455s/iter; left time: 34.7893s
Epoch: 3 cost time: 3.959264039993286
Epoch: 3, Steps: 108 | Train Loss: 113.6040035 Vali Loss: 11.6587067 Test Loss: 11.7406635
Validation loss decreased (11.687102 --> 11.658707).  Saving model ...
not in xxxx...
type1 => Adjust updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 23.6160088
	speed: 0.0420s/iter; left time: 27.6076s
Epoch: 4 cost time: 3.3369438648223877
Epoch: 4, Steps: 108 | Train Loss: 113.1049159 Vali Loss: 11.6466227 Test Loss: 11.7310753
Validation loss decreased (11.658707 --> 11.646623).  Saving model ...
not in xxxx...
type1 => Adjust updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 45.7632713
	speed: 0.0357s/iter; left time: 19.6234s
Epoch: 5 cost time: 2.838611125946045
Epoch: 5, Steps: 108 | Train Loss: 93.4622386 Vali Loss: 11.6186914 Test Loss: 11.7228413
Validation loss decreased (11.646623 --> 11.618691).  Saving model ...
not in xxxx...
type1 => Adjust updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 407.7287292
	speed: 0.0353s/iter; left time: 15.5603s
Epoch: 6 cost time: 2.825596332550049
Epoch: 6, Steps: 108 | Train Loss: 106.9568729 Vali Loss: 11.6119118 Test Loss: 11.7152271
Validation loss decreased (11.618691 --> 11.611912).  Saving model ...
not in xxxx...
type1 => Adjust updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 19.4571457
	speed: 0.0339s/iter; left time: 11.3009s
Epoch: 7 cost time: 2.8191564083099365
Epoch: 7, Steps: 108 | Train Loss: 228.3768930 Vali Loss: 11.6015558 Test Loss: 11.7235231
Validation loss decreased (11.611912 --> 11.601556).  Saving model ...
not in xxxx...
type1 => Adjust updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 32.2954369
	speed: 0.0355s/iter; left time: 7.9870s
Epoch: 8 cost time: 2.663853645324707
Epoch: 8, Steps: 108 | Train Loss: 169.7168902 Vali Loss: 11.5964537 Test Loss: 11.7216091
Validation loss decreased (11.601556 --> 11.596454).  Saving model ...
not in xxxx...
type1 => Adjust updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 20.7449188
	speed: 0.0355s/iter; left time: 4.1587s
Epoch: 9 cost time: 2.867074966430664
Epoch: 9, Steps: 108 | Train Loss: 176.0619849 Vali Loss: 11.5948114 Test Loss: 11.7206707
Validation loss decreased (11.596454 --> 11.594811).  Saving model ...
not in xxxx...
type1 => Adjust updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 1414.7531738
	speed: 0.0468s/iter; left time: 0.4216s
Epoch: 10 cost time: 4.191936254501343
Epoch: 10, Steps: 108 | Train Loss: 177.8130848 Vali Loss: 11.5945578 Test Loss: 11.7205000
Validation loss decreased (11.594811 --> 11.594558).  Saving model ...
not in xxxx...
type1 => Adjust updating learning rate to 1.953125e-07
>>>>>>>testing : 8_5_Autoformer_C2TM_ftM_sl8_ll4_pl5_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
data shape:  (48, 13269) 13269
adj_mx shape:  (13269, 13269)
loading model.............
mse:11.720500946044922, mae:1.0096133947372437, rse:1.132050633430481
